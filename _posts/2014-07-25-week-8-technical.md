---
layout: post
title:  "Week 8 Technical Blog"
date:   2014-07-25
categories: technical
---

Week 8:
=======

How to Program a Programming Language
-------------------------------------

How does everything we’re working on… work? Where did Ruby come from, as in how do you program a programming language? For that matter, how do you program a computer? How does a motherboard translate electrical impulses from my keyboard to present the words I type on the screen?

Let’s take a look back, shall we?

So, let me make some analogies. We have computers and programming languages. Programming languages are just like the language we speak in our every day lives. As humans, we invented languages to communicate between each other. A programming language is another invention we’ve created to communicate between two types of entities, humans (or users), and computers. In that way, if you think about it, a computer is a prototypical human. Now that we can communicate with computers, we can teach them to do many things. 

Let’s go back to that prototypical human to trace how programs and computers came to be. Our brains weren’t developed enough to process language for a long period of human evolution. Sure dogs, cats, monkeys, all animals have their own “language”, but it’s incredibly basic and instinctual. The prototypical human version of the computer is actually a very early calculator.

In 1837, Charles Babbage designed the “Analytical Engine”, or what most people consider the first “modern” computer. It worked by a method of punched cards that had been introduced in 1801 by Joseph Marie Jacquard in his Jacquard Loom. A card punched in a particular pattern would be connected to strings on a loom through hooks, and whether or not the card was punched in a specific place would raise or lower the hooks— a series of cards punched in different places could create beautiful patterns. Babbage’s design of the Analytical machine was to take input punched cards and return a print. The Analytical Engine could handle basic arithmetic through conditional branching and loops (Sound familiar? More on this [here](http://en.wikipedia.org/wiki/Analytical_engine#Instruction_set)) and had integrated memory (numerical constants). Six years later, Ada Lovelace (girl power!) took a description of the machine and translated it, but also added her own notes as to how to repurpose the machine to calculate Bernoulli numbers. She invented a new way to communicate between a machine and its user, and while it was not technically a programming language, the first programming language ever is named after her. 

So, now we start to see how we can get from this: 

![Punchcard Image 1]({{site.url}}/assets/punchcards-large.jpg)

to this:
				
![Punchcard Image 2]({{site.url}}/site/ibm-80-column-punched-card1.jpg)

and then to this: *(though it would take a whole different blog post...)*

![Punchard Image 3]({{site.url}}/site/football5.jpg)

And we know where it goes from there... 

So if humans evolved to process language, computers evolved from processing punched cards to computer chips. Programming languages evolved from a binary system in weaving (i.e. punched or not punched pulled the thread above or below the waft), to incorporate math and loops and booleans further and further into the Rubys, SQLs, JavaScripts, and HTMLs we know and love today.